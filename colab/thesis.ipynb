{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VL_4GSo7lRWX"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from numpy import save\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "COLAB=True\n",
    "#WORKDIR=\"drive/My Drive/Colab Notebooks/data\"\n",
    "WORKDIR=\"./data\"\n",
    "INITIAL_EPOCH=201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgYKrYcRpDMS"
   },
   "outputs": [],
   "source": [
    "def make_colab_env():\n",
    "  os.mkdir(os.path.join(WORKDIR, \"clean\"))\n",
    "  os.mkdir(os.path.join(WORKDIR, \"noisy\"))\n",
    "  os.mkdir(os.path.join(WORKDIR, \"checkpoints\"))\n",
    "  os.mkdir(os.path.join(WORKDIR, \"logs\"))\n",
    "  noisy_samples_directory = os.mkdir(os.path.join(WORKDIR, \"noisy-samplified\"))\n",
    "  clean_samples_directory = os.mkdir(os.path.join(WORKDIR, \"clean-samplified\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G976t5H_qzHn"
   },
   "outputs": [],
   "source": [
    "# update librosa in colab\n",
    "!pip install librosa==0.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqUvFoFLloxR"
   },
   "outputs": [],
   "source": [
    "def read_noise(audio_files: List[str], sampling: int, frame_length: int, hop_length: float = 0.4) -> np.ndarray:\n",
    "    audio_stacks = []\n",
    "    total = len(audio_files)\n",
    "    for i, f in enumerate(audio_files):\n",
    "        y, _ = librosa.load(f, sr=sampling)\n",
    "        audio_stacks.append(librosa.util.frame(y, frame_length=frame_length, hop_length=int(frame_length * hop_length), axis=0))\n",
    "        print(\"Reading noise [{0} / {1}]\".format(i+1, total))\n",
    "    random.shuffle(audio_stacks)\n",
    "    return np.vstack(audio_stacks)\n",
    "\n",
    "# Blend speech with noise\n",
    "def create_audio_samples(noise_dir: str, speech_dir: str, noisy_dir: str, clean_dir: str,\n",
    "           frame_length: int, sampling: int, noise_frame_hop: float = 0.4) -> int:\n",
    "    \n",
    "    noise_files = list(os.scandir(noise_dir))\n",
    "    speech_files = list(os.scandir(speech_dir))\n",
    "\n",
    "    random.shuffle(noise_files)\n",
    "    random.shuffle(speech_files)\n",
    "    \n",
    "    noise_files = noise_files[:10]\n",
    "\n",
    "    print(\"Reading noise into memory\")\n",
    "    noise_frames = read_noise(noise_files, sampling,\n",
    "                              int(frame_length/2), noise_frame_hop)\n",
    "\n",
    "    samples_count = 0\n",
    "    total = len(speech_files)\n",
    "    for idx, sample_file in enumerate(speech_files):\n",
    "        y, _ = librosa.load(sample_file, sr=sampling)\n",
    "        if len(y) < frame_length:\n",
    "            print(\"Dropping {0} because is shorter than frame_length\".format(sample_file))\n",
    "            continue\n",
    "        y = librosa.util.frame(y, frame_length=frame_length, hop_length=frame_length, axis=0)\n",
    "        samples_count += y.shape[0]\n",
    "        filename = os.path.splitext(os.path.basename(sample_file))[0] + \".wav\"\n",
    "        librosa.output.write_wav(os.path.join(clean_dir, filename), y.reshape(1, -1)[0], sr=sampling)\n",
    "        for i in range(y.shape[0]):\n",
    "            y[i, :] += random.uniform(0.2, 0.5) * np.concatenate((random.choice(noise_frames),random.choice(noise_frames)))\n",
    "        librosa.output.write_wav(os.path.join(noisy_dir, filename), y.reshape(1, -1)[0], sr=sampling)\n",
    "        print(\"Blending noise with sample speech [{0} / {1}]\".format(idx, total))\n",
    "\n",
    "    print(\"Possible {0} samples\".format(samples_count))\n",
    "    return samples_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRaXspeVnweH"
   },
   "outputs": [],
   "source": [
    "if not COLAB:\n",
    "  noise_directory = \"/home/michal/ESC-50/ESC-50-master/audio\"\n",
    "  speech_directory = \"/home/michal/OpenSLR/LibriSpeech/dev-clean/\"\n",
    "  noisy_directory = \"/home/michal/Documents/Speech-enhancement/data/noisy\"\n",
    "  clean_directory = \"/home/michal/Documents/Speech-enhancement/data/clean\"\n",
    "  noisy_samples_directory=\"/home/michal/Documents/Speech-enhancement/data/noisy-samplified\"\n",
    "  clean_samples_directory=\"/home/michal/Documents/Speech-enhancement/data/clean-samplified\"\n",
    "  scaler_path = \"/home/michal/Documents/Speech-enhancement/data/scaler.plk\"\n",
    "else:\n",
    "  noise_directory = \"drive/My Drive/Colab Notebooks/ESC-50-master/audio\"\n",
    "  speech_directory = \"drive/My Drive/Colab Notebooks/LibriSpeech/dev-clean/\"\n",
    "  noisy_directory = os.path.join(WORKDIR, \"noisy\")\n",
    "  clean_directory = os.path.join(WORKDIR, \"clean\")\n",
    "  noisy_samples_directory = os.path.join(WORKDIR, \"noisy-samplified\")\n",
    "  clean_samples_directory = os.path.join(WORKDIR, \"clean-samplified\")\n",
    "  scaler_path = os.path.join(WORKDIR, \"scaler.plk\")\n",
    "  checkpoints_path = os.path.join(WORKDIR, \"checkpoints\")\n",
    "  logs_dir = os.path.join(WORKDIR, \"logs\")\n",
    "\n",
    "npy_samples_count = 100000\n",
    "frame_length = 16384\n",
    "sampling = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aCzzS9goyaY"
   },
   "outputs": [],
   "source": [
    "#create_audio_samples(noise_directory,\n",
    "#                    speech_directory,\n",
    "#                    noisy_directory,\n",
    "#                    clean_directory,\n",
    "#                    frame_length,\n",
    "#                    sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVSH2S_fpsYh"
   },
   "outputs": [],
   "source": [
    "def samplify(audio_files: List[str], output_path: str, \n",
    "             frame_length: int, sampling: int, npy_samples_count: int):\n",
    "    npy_frames = []\n",
    "    npy_idx = 0\n",
    "    samples_in_npy_frames = 0\n",
    "    for audio_idx, audio_file in enumerate(audio_files):\n",
    "        y, _ = librosa.load(audio_file, sr=sampling)\n",
    "        frames = librosa.util.frame(\n",
    "            y, frame_length=frame_length, hop_length=int(frame_length * 0.3), axis=0)\n",
    "        samples_in_npy_frames += frames.shape[0]\n",
    "        npy_frames.append(frames)\n",
    "        if samples_in_npy_frames > npy_samples_count:\n",
    "            output = os.path.join(output_path, \"{}.npy\".format(npy_idx))\n",
    "            v = np.vstack(npy_frames)\n",
    "            print(\"\\nWriting {0} into {1}\".format(v.shape, output))\n",
    "            save(output, v)\n",
    "            npy_idx += 1\n",
    "            npy_frames = []\n",
    "            samples_in_npy_frames = 0\n",
    "        print(\"Samplifying [{0} / {1}]\".format(audio_idx+1, len(audio_files)))\n",
    "    if len(npy_frames) > 0:\n",
    "        output = os.path.join(output_path, \"{}.npy\".format(npy_idx))\n",
    "        v = np.vstack(npy_frames)\n",
    "        print(\"\\nWriting {0} into {1}\".format(v.shape, output))\n",
    "        save(output, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ZMi4RvWrXEn"
   },
   "outputs": [],
   "source": [
    "def fit_scaler(samples_npy: List[str], scaler_save_path: str):\n",
    "    scaler = MinMaxScaler()\n",
    "    for sample_idx, samples_path in enumerate(samples_npy):\n",
    "        print(\"Fitting scaler [{0} / {1}]\".format(sample_idx+1, len(samples_npy)))\n",
    "        samples = np.load(samples_path, allow_pickle=True)\n",
    "        shape = samples.shape\n",
    "        scaler.partial_fit(samples.reshape(shape[0], -1))\n",
    "    print(\"Saving scaler to %s\" % scaler_save_path)\n",
    "    with open(scaler_save_path, 'wb+') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "\n",
    "def scale_it(samples_npy: List[str], scaler_path: str):\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    for sample_idx, samples_path in enumerate(samples_npy):\n",
    "        print(\"Scaling samples [{0} / {1}]\".format(sample_idx+1, len(samples_npy)))\n",
    "        samples = np.load(samples_path)\n",
    "        shape = samples.shape\n",
    "        samples = samples.reshape(shape[0], -1)\n",
    "        samples = scaler.transform(samples).reshape(shape)\n",
    "        temp_output_file = samples_path.split('.')[0] + \"_tmp.npy\"\n",
    "        save(temp_output_file, samples)\n",
    "        os.rename(temp_output_file, samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAS_8Rr7r09k"
   },
   "outputs": [],
   "source": [
    "#samplify(list(os.scandir(noisy_directory)), noisy_samples_directory, frame_length, sampling, npy_samples_count)\n",
    "#samplify(list(os.scandir(clean_directory)), clean_samples_directory, frame_length, sampling, npy_samples_count)\n",
    "#noise_npy_samples = [p.path for p in os.scandir(noisy_samples_directory)] \n",
    "#clean_npy_samples = [p.path for p in os.scandir(clean_samples_directory)]\n",
    "#fit_scaler(noise_npy_samples+clean_npy_samples, scaler_path)\n",
    "#scale_it(noise_npy_samples+clean_npy_samples, scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4jULFCusRpg"
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_npy_files: List[str], y_npy_files: List[str], batch_size: int, shuffle: bool = False):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.sample_count = 0\n",
    "        for npy_file in x_npy_files:\n",
    "            shape = np.load(npy_file).shape\n",
    "            self.sample_count += shape[0]\n",
    "\n",
    "        self.x_npy_files = x_npy_files\n",
    "        self.y_npy_files = y_npy_files\n",
    "\n",
    "        self._on_each_epoch()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.floor(self.sample_count / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index == 0:\n",
    "            self._on_each_epoch()\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for _ in range(0, self.batch_size):\n",
    "            x_batch.append(next(self.x_generator))\n",
    "            y_batch.append(next(self.y_generator))\n",
    "        if self.shuffle:\n",
    "            indexes = list(range(0, len(x_batch)))\n",
    "            random.shuffle(indexes)\n",
    "            for i, j in enumerate(indexes):\n",
    "                x_batch[i], x_batch[j] = x_batch[j], x_batch[i]\n",
    "                y_batch[i], y_batch[j] = y_batch[j], y_batch[i]\n",
    "        return np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "    def _on_each_epoch(self):\n",
    "        self.x_samples_list = map(np.load, self.x_npy_files)\n",
    "        self.x_samples_list = map(lambda m: m.reshape(\n",
    "            m.shape[0], m.shape[1], 1), self.x_samples_list)\n",
    "\n",
    "        self.y_samples_list = map(np.load, self.y_npy_files)\n",
    "        self.y_samples_list = map(lambda m: m.reshape(\n",
    "            m.shape[0], m.shape[1], 1), self.y_samples_list)\n",
    "\n",
    "        def lazy_numpy_vstack(matrices):\n",
    "            for matrix in matrices:\n",
    "                yield from matrix\n",
    "\n",
    "        self.x_generator = lazy_numpy_vstack(self.x_samples_list)\n",
    "        self.y_generator = lazy_numpy_vstack(self.y_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yux9DzJEu8pV"
   },
   "outputs": [],
   "source": [
    "#G = Generator(list(os.scandir(noisy_samples_directory)), list(os.scandir(clean_samples_directory)), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kY_83KKBvHxf"
   },
   "outputs": [],
   "source": [
    "def LeakyReLU(x, alpha=0.3):\n",
    "    return tf.maximum(alpha*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQ7sEEAfvJwU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Conv1D, Reshape, UpSampling1D, concatenate\n",
    "\n",
    "def get_unet():\n",
    "    initial_filters=24\n",
    "    kernel_downsampling=15\n",
    "    kernel_upsampling=5\n",
    "    padding='same'\n",
    "    layer_factors = [1, 2, 4, 8, 16]\n",
    "\n",
    "    encoder_layers = []\n",
    "\n",
    "    inputs = Input((16384,1))\n",
    "    x = inputs\n",
    "    for i in layer_factors[:-1]:\n",
    "        x = tf.keras.layers.Conv1D(initial_filters * i, kernel_downsampling, strides=1, activation=LeakyReLU, padding=padding)(x)\n",
    "        encoder_layers.append(x)\n",
    "        x = x[:,::2,:]\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(initial_filters * layer_factors[-1], kernel_downsampling, strides=1, activation=LeakyReLU, padding=padding)(x)\n",
    "\n",
    "    for i, el in zip(reversed(layer_factors[:-1]), reversed(encoder_layers)):\n",
    "        x = UpSampling1D()(x)\n",
    "        x = concatenate([x, el])\n",
    "        x = tf.keras.layers.Conv1D(initial_filters * i, kernel_upsampling, strides=1, activation=LeakyReLU, padding=padding)(x)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(1, kernel_upsampling, strides=1, activation=tf.tanh, padding=padding)(x)\n",
    "\n",
    "    outputs=x\n",
    "\n",
    "    model = Model(inputs, outputs=outputs, name=\"example\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10144,
     "status": "ok",
     "timestamp": 1590503110375,
     "user": {
      "displayName": "Michał Pawłowicz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPkH6a41e0KpMNM5QJiFitPGK504YR3M3KzwBX=s64",
      "userId": "13082843782255968248"
     },
     "user_tz": -120
    },
    "id": "drLAFopTvMiR",
    "outputId": "960a23b0-815d-4aa5-e306-c90898d5a8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/michal/Documents/Speech-enhancement/env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/michal/Documents/Speech-enhancement/env/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"example\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 16384, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 16384, 24)    384         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te (None, 8192, 24)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 8192, 48)     17328       tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te (None, 4096, 48)     0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 4096, 96)     69216       tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T (None, 2048, 96)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 2048, 192)    276672      tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T (None, 1024, 192)    0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1024, 384)    1106304     tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_8 (UpSampling1D)  (None, 2048, 384)    0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 2048, 576)    0           up_sampling1d_8[0][0]            \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 2048, 192)    553152      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_9 (UpSampling1D)  (None, 4096, 192)    0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 4096, 288)    0           up_sampling1d_9[0][0]            \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 4096, 96)     138336      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_10 (UpSampling1D) (None, 8192, 96)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8192, 144)    0           up_sampling1d_10[0][0]           \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 8192, 48)     34608       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_11 (UpSampling1D) (None, 16384, 48)    0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16384, 72)    0           up_sampling1d_11[0][0]           \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 16384, 24)    8664        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 16384, 1)     121         conv1d_28[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,204,785\n",
      "Trainable params: 2,204,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "#model = get_unet()\n",
    "model = tf.keras.models.load_model(os.path.join(WORKDIR, \"checkpoints\", \"model-cp-epoch_0201.h5\"), \n",
    "                                   custom_objects={\"LeakyReLU\" : LeakyReLU})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47722,
     "status": "ok",
     "timestamp": 1590503152728,
     "user": {
      "displayName": "Michał Pawłowicz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPkH6a41e0KpMNM5QJiFitPGK504YR3M3KzwBX=s64",
      "userId": "13082843782255968248"
     },
     "user_tz": -120
    },
    "id": "ZwUdZ24fvOC0",
    "outputId": "c0f90f46-2600-4e5a-92f8-1c084d59e009"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load(os.path.join(noisy_samples_directory, \"0.npy\"))\n",
    "X = X.reshape(X.shape[0],X.shape[1],1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61295,
     "status": "ok",
     "timestamp": 1590503214039,
     "user": {
      "displayName": "Michał Pawłowicz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPkH6a41e0KpMNM5QJiFitPGK504YR3M3KzwBX=s64",
      "userId": "13082843782255968248"
     },
     "user_tz": -120
    },
    "id": "VMkziSYKSDVq",
    "outputId": "adcd7e9b-1f32-461e-8e79-f4088377e252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.load(os.path.join(clean_samples_directory, \"0.npy\"))\n",
    "Y = Y.reshape(Y.shape[0],Y.shape[1],1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiwaDvavSKl1"
   },
   "outputs": [],
   "source": [
    "indexes = list(range(0, X.shape[0]))\n",
    "random.shuffle(indexes)\n",
    "for i, j in enumerate(indexes):\n",
    "  X[i], X[j] = X[j], X[i]\n",
    "  Y[i], Y[j] = Y[j], Y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14798,
     "status": "error",
     "timestamp": 1590348420242,
     "user": {
      "displayName": "Michał Pawłowicz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPkH6a41e0KpMNM5QJiFitPGK504YR3M3KzwBX=s64",
      "userId": "13082843782255968248"
     },
     "user_tz": -120
    },
    "id": "kx_GrRMtvPjt",
    "outputId": "19f23357-c546-4be7-abf9-29e8a1acaf4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train on 51403 samples\n",
      "Epoch 202/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 6.2597e-05\n",
      "Epoch 00202: saving model to ./data/checkpoints/model-cp-epoch_0202.h5\n",
      "51403/51403 [==============================] - 479s 9ms/sample - loss: 6.2601e-05\n",
      "Epoch 203/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.8932e-05\n",
      "Epoch 00203: saving model to ./data/checkpoints/model-cp-epoch_0203.h5\n",
      "51403/51403 [==============================] - 462s 9ms/sample - loss: 5.8934e-05\n",
      "Epoch 204/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.7522e-05\n",
      "Epoch 00204: saving model to ./data/checkpoints/model-cp-epoch_0204.h5\n",
      "51403/51403 [==============================] - 458s 9ms/sample - loss: 5.7526e-05\n",
      "Epoch 205/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.7336e-05\n",
      "Epoch 00205: saving model to ./data/checkpoints/model-cp-epoch_0205.h5\n",
      "51403/51403 [==============================] - 458s 9ms/sample - loss: 5.7341e-05\n",
      "Epoch 206/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.6868e-05\n",
      "Epoch 00206: saving model to ./data/checkpoints/model-cp-epoch_0206.h5\n",
      "51403/51403 [==============================] - 458s 9ms/sample - loss: 5.6877e-05\n",
      "Epoch 207/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.5672e-05\n",
      "Epoch 00207: saving model to ./data/checkpoints/model-cp-epoch_0207.h5\n",
      "51403/51403 [==============================] - 458s 9ms/sample - loss: 5.5676e-05\n",
      "Epoch 208/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.5634e-05\n",
      "Epoch 00208: saving model to ./data/checkpoints/model-cp-epoch_0208.h5\n",
      "51403/51403 [==============================] - 459s 9ms/sample - loss: 5.5634e-05\n",
      "Epoch 209/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.5142e-05\n",
      "Epoch 00209: saving model to ./data/checkpoints/model-cp-epoch_0209.h5\n",
      "51403/51403 [==============================] - 456s 9ms/sample - loss: 5.5142e-05\n",
      "Epoch 210/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.4127e-05\n",
      "Epoch 00210: saving model to ./data/checkpoints/model-cp-epoch_0210.h5\n",
      "51403/51403 [==============================] - 457s 9ms/sample - loss: 5.4124e-05\n",
      "Epoch 211/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.4128e-05\n",
      "Epoch 00211: saving model to ./data/checkpoints/model-cp-epoch_0211.h5\n",
      "51403/51403 [==============================] - 456s 9ms/sample - loss: 5.4129e-05\n",
      "Epoch 212/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.3363e-05\n",
      "Epoch 00212: saving model to ./data/checkpoints/model-cp-epoch_0212.h5\n",
      "51403/51403 [==============================] - 456s 9ms/sample - loss: 5.3363e-05\n",
      "Epoch 213/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.2865e-05\n",
      "Epoch 00213: saving model to ./data/checkpoints/model-cp-epoch_0213.h5\n",
      "51403/51403 [==============================] - 456s 9ms/sample - loss: 5.2865e-05\n",
      "Epoch 214/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.2934e-05\n",
      "Epoch 00214: saving model to ./data/checkpoints/model-cp-epoch_0214.h5\n",
      "51403/51403 [==============================] - 457s 9ms/sample - loss: 5.2939e-05\n",
      "Epoch 215/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.2769e-05\n",
      "Epoch 00215: saving model to ./data/checkpoints/model-cp-epoch_0215.h5\n",
      "51403/51403 [==============================] - 459s 9ms/sample - loss: 5.2771e-05\n",
      "Epoch 216/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.1927e-05\n",
      "Epoch 00216: saving model to ./data/checkpoints/model-cp-epoch_0216.h5\n",
      "51403/51403 [==============================] - 459s 9ms/sample - loss: 5.1926e-05\n",
      "Epoch 217/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.1688e-05\n",
      "Epoch 00217: saving model to ./data/checkpoints/model-cp-epoch_0217.h5\n",
      "51403/51403 [==============================] - 458s 9ms/sample - loss: 5.1690e-05\n",
      "Epoch 218/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.1834e-05\n",
      "Epoch 00218: saving model to ./data/checkpoints/model-cp-epoch_0218.h5\n",
      "51403/51403 [==============================] - 456s 9ms/sample - loss: 5.1835e-05\n",
      "Epoch 219/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.0964e-05\n",
      "Epoch 00219: saving model to ./data/checkpoints/model-cp-epoch_0219.h5\n",
      "51403/51403 [==============================] - 454s 9ms/sample - loss: 5.0964e-05\n",
      "Epoch 220/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.0951e-05\n",
      "Epoch 00220: saving model to ./data/checkpoints/model-cp-epoch_0220.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 5.0952e-05\n",
      "Epoch 221/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.1861e-05\n",
      "Epoch 00221: saving model to ./data/checkpoints/model-cp-epoch_0221.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 5.1861e-05\n",
      "Epoch 222/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 5.0041e-05\n",
      "Epoch 00222: saving model to ./data/checkpoints/model-cp-epoch_0222.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 5.0038e-05\n",
      "Epoch 223/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.9947e-05\n",
      "Epoch 00223: saving model to ./data/checkpoints/model-cp-epoch_0223.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.9950e-05\n",
      "Epoch 224/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.9791e-05\n",
      "Epoch 00224: saving model to ./data/checkpoints/model-cp-epoch_0224.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.9791e-05\n",
      "Epoch 225/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.9369e-05\n",
      "Epoch 00225: saving model to ./data/checkpoints/model-cp-epoch_0225.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.9367e-05\n",
      "Epoch 226/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.9522e-05\n",
      "Epoch 00226: saving model to ./data/checkpoints/model-cp-epoch_0226.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.9524e-05\n",
      "Epoch 227/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.8780e-05\n",
      "Epoch 00227: saving model to ./data/checkpoints/model-cp-epoch_0227.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.8779e-05\n",
      "Epoch 228/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.9037e-05\n",
      "Epoch 00228: saving model to ./data/checkpoints/model-cp-epoch_0228.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.9034e-05\n",
      "Epoch 229/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.8719e-05\n",
      "Epoch 00229: saving model to ./data/checkpoints/model-cp-epoch_0229.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.8720e-05\n",
      "Epoch 230/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.8397e-05\n",
      "Epoch 00230: saving model to ./data/checkpoints/model-cp-epoch_0230.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.8397e-05\n",
      "Epoch 231/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.8082e-05\n",
      "Epoch 00231: saving model to ./data/checkpoints/model-cp-epoch_0231.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.8081e-05\n",
      "Epoch 232/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.7894e-05\n",
      "Epoch 00232: saving model to ./data/checkpoints/model-cp-epoch_0232.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.7891e-05\n",
      "Epoch 233/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.7860e-05\n",
      "Epoch 00233: saving model to ./data/checkpoints/model-cp-epoch_0233.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.7859e-05\n",
      "Epoch 234/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.7462e-05\n",
      "Epoch 00234: saving model to ./data/checkpoints/model-cp-epoch_0234.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.7466e-05\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.7253e-05\n",
      "Epoch 00235: saving model to ./data/checkpoints/model-cp-epoch_0235.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.7249e-05\n",
      "Epoch 236/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.7272e-05\n",
      "Epoch 00236: saving model to ./data/checkpoints/model-cp-epoch_0236.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.7272e-05\n",
      "Epoch 237/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.6811e-05\n",
      "Epoch 00237: saving model to ./data/checkpoints/model-cp-epoch_0237.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.6811e-05\n",
      "Epoch 238/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.6867e-05\n",
      "Epoch 00238: saving model to ./data/checkpoints/model-cp-epoch_0238.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.6868e-05\n",
      "Epoch 239/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.6816e-05\n",
      "Epoch 00239: saving model to ./data/checkpoints/model-cp-epoch_0239.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.6812e-05\n",
      "Epoch 240/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.6325e-05\n",
      "Epoch 00240: saving model to ./data/checkpoints/model-cp-epoch_0240.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.6323e-05\n",
      "Epoch 241/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.6109e-05\n",
      "Epoch 00241: saving model to ./data/checkpoints/model-cp-epoch_0241.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.6110e-05\n",
      "Epoch 242/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.6440e-05\n",
      "Epoch 00242: saving model to ./data/checkpoints/model-cp-epoch_0242.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.6439e-05\n",
      "Epoch 243/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.5989e-05\n",
      "Epoch 00243: saving model to ./data/checkpoints/model-cp-epoch_0243.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.5987e-05\n",
      "Epoch 244/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.5410e-05\n",
      "Epoch 00244: saving model to ./data/checkpoints/model-cp-epoch_0244.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.5414e-05\n",
      "Epoch 245/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.6009e-05\n",
      "Epoch 00245: saving model to ./data/checkpoints/model-cp-epoch_0245.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.6011e-05\n",
      "Epoch 246/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.5911e-05\n",
      "Epoch 00246: saving model to ./data/checkpoints/model-cp-epoch_0246.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.5908e-05\n",
      "Epoch 247/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.5217e-05\n",
      "Epoch 00247: saving model to ./data/checkpoints/model-cp-epoch_0247.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.5217e-05\n",
      "Epoch 248/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.5387e-05\n",
      "Epoch 00248: saving model to ./data/checkpoints/model-cp-epoch_0248.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.5390e-05\n",
      "Epoch 249/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4828e-05\n",
      "Epoch 00249: saving model to ./data/checkpoints/model-cp-epoch_0249.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.4827e-05\n",
      "Epoch 250/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4843e-05\n",
      "Epoch 00250: saving model to ./data/checkpoints/model-cp-epoch_0250.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.4841e-05\n",
      "Epoch 251/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4961e-05\n",
      "Epoch 00251: saving model to ./data/checkpoints/model-cp-epoch_0251.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.4958e-05\n",
      "Epoch 252/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4817e-05\n",
      "Epoch 00252: saving model to ./data/checkpoints/model-cp-epoch_0252.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.4817e-05\n",
      "Epoch 253/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4398e-05\n",
      "Epoch 00253: saving model to ./data/checkpoints/model-cp-epoch_0253.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.4398e-05\n",
      "Epoch 254/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4746e-05\n",
      "Epoch 00254: saving model to ./data/checkpoints/model-cp-epoch_0254.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.4746e-05\n",
      "Epoch 255/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4250e-05\n",
      "Epoch 00255: saving model to ./data/checkpoints/model-cp-epoch_0255.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.4252e-05\n",
      "Epoch 256/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.4692e-05\n",
      "Epoch 00256: saving model to ./data/checkpoints/model-cp-epoch_0256.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.4703e-05\n",
      "Epoch 257/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3772e-05\n",
      "Epoch 00257: saving model to ./data/checkpoints/model-cp-epoch_0257.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.3771e-05\n",
      "Epoch 258/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3811e-05\n",
      "Epoch 00258: saving model to ./data/checkpoints/model-cp-epoch_0258.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.3819e-05\n",
      "Epoch 259/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3783e-05\n",
      "Epoch 00259: saving model to ./data/checkpoints/model-cp-epoch_0259.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.3779e-05\n",
      "Epoch 260/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3705e-05\n",
      "Epoch 00260: saving model to ./data/checkpoints/model-cp-epoch_0260.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.3707e-05\n",
      "Epoch 261/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3866e-05\n",
      "Epoch 00261: saving model to ./data/checkpoints/model-cp-epoch_0261.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.3864e-05\n",
      "Epoch 262/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3079e-05\n",
      "Epoch 00262: saving model to ./data/checkpoints/model-cp-epoch_0262.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.3078e-05\n",
      "Epoch 263/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3627e-05\n",
      "Epoch 00263: saving model to ./data/checkpoints/model-cp-epoch_0263.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.3626e-05\n",
      "Epoch 264/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3096e-05\n",
      "Epoch 00264: saving model to ./data/checkpoints/model-cp-epoch_0264.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.3096e-05\n",
      "Epoch 265/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3101e-05\n",
      "Epoch 00265: saving model to ./data/checkpoints/model-cp-epoch_0265.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.3102e-05\n",
      "Epoch 266/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.3200e-05\n",
      "Epoch 00266: saving model to ./data/checkpoints/model-cp-epoch_0266.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.3199e-05\n",
      "Epoch 267/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2705e-05\n",
      "Epoch 00267: saving model to ./data/checkpoints/model-cp-epoch_0267.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.2704e-05\n",
      "Epoch 268/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2759e-05\n",
      "Epoch 00268: saving model to ./data/checkpoints/model-cp-epoch_0268.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.2758e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 269/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2585e-05\n",
      "Epoch 00269: saving model to ./data/checkpoints/model-cp-epoch_0269.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.2585e-05\n",
      "Epoch 270/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2656e-05\n",
      "Epoch 00270: saving model to ./data/checkpoints/model-cp-epoch_0270.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.2658e-05\n",
      "Epoch 271/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2278e-05\n",
      "Epoch 00271: saving model to ./data/checkpoints/model-cp-epoch_0271.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.2279e-05\n",
      "Epoch 272/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2247e-05\n",
      "Epoch 00272: saving model to ./data/checkpoints/model-cp-epoch_0272.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.2249e-05\n",
      "Epoch 273/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2315e-05\n",
      "Epoch 00273: saving model to ./data/checkpoints/model-cp-epoch_0273.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.2312e-05\n",
      "Epoch 274/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2212e-05\n",
      "Epoch 00274: saving model to ./data/checkpoints/model-cp-epoch_0274.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.2210e-05\n",
      "Epoch 275/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2196e-05\n",
      "Epoch 00275: saving model to ./data/checkpoints/model-cp-epoch_0275.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.2194e-05\n",
      "Epoch 276/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2091e-05\n",
      "Epoch 00276: saving model to ./data/checkpoints/model-cp-epoch_0276.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.2090e-05\n",
      "Epoch 277/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2120e-05\n",
      "Epoch 00277: saving model to ./data/checkpoints/model-cp-epoch_0277.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.2118e-05\n",
      "Epoch 278/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1836e-05\n",
      "Epoch 00278: saving model to ./data/checkpoints/model-cp-epoch_0278.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.1834e-05\n",
      "Epoch 279/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1566e-05\n",
      "Epoch 00279: saving model to ./data/checkpoints/model-cp-epoch_0279.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.1569e-05\n",
      "Epoch 280/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.2391e-05\n",
      "Epoch 00280: saving model to ./data/checkpoints/model-cp-epoch_0280.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.2395e-05\n",
      "Epoch 281/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1849e-05\n",
      "Epoch 00281: saving model to ./data/checkpoints/model-cp-epoch_0281.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.1851e-05\n",
      "Epoch 282/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1428e-05\n",
      "Epoch 00282: saving model to ./data/checkpoints/model-cp-epoch_0282.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.1429e-05\n",
      "Epoch 283/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1285e-05\n",
      "Epoch 00283: saving model to ./data/checkpoints/model-cp-epoch_0283.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.1292e-05\n",
      "Epoch 284/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1353e-05\n",
      "Epoch 00284: saving model to ./data/checkpoints/model-cp-epoch_0284.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.1354e-05\n",
      "Epoch 285/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1668e-05\n",
      "Epoch 00285: saving model to ./data/checkpoints/model-cp-epoch_0285.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.1665e-05\n",
      "Epoch 286/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0865e-05\n",
      "Epoch 00286: saving model to ./data/checkpoints/model-cp-epoch_0286.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0867e-05\n",
      "Epoch 287/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.1152e-05\n",
      "Epoch 00287: saving model to ./data/checkpoints/model-cp-epoch_0287.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.1152e-05\n",
      "Epoch 288/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0930e-05\n",
      "Epoch 00288: saving model to ./data/checkpoints/model-cp-epoch_0288.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0930e-05\n",
      "Epoch 289/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0807e-05\n",
      "Epoch 00289: saving model to ./data/checkpoints/model-cp-epoch_0289.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0807e-05\n",
      "Epoch 290/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0804e-05\n",
      "Epoch 00290: saving model to ./data/checkpoints/model-cp-epoch_0290.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.0804e-05\n",
      "Epoch 291/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0765e-05\n",
      "Epoch 00291: saving model to ./data/checkpoints/model-cp-epoch_0291.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0767e-05\n",
      "Epoch 292/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0726e-05\n",
      "Epoch 00292: saving model to ./data/checkpoints/model-cp-epoch_0292.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.0729e-05\n",
      "Epoch 293/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0377e-05\n",
      "Epoch 00293: saving model to ./data/checkpoints/model-cp-epoch_0293.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.0377e-05\n",
      "Epoch 294/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0434e-05\n",
      "Epoch 00294: saving model to ./data/checkpoints/model-cp-epoch_0294.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0433e-05\n",
      "Epoch 295/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0313e-05\n",
      "Epoch 00295: saving model to ./data/checkpoints/model-cp-epoch_0295.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0310e-05\n",
      "Epoch 296/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0766e-05\n",
      "Epoch 00296: saving model to ./data/checkpoints/model-cp-epoch_0296.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0768e-05\n",
      "Epoch 297/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0255e-05\n",
      "Epoch 00297: saving model to ./data/checkpoints/model-cp-epoch_0297.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.0255e-05\n",
      "Epoch 298/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0218e-05\n",
      "Epoch 00298: saving model to ./data/checkpoints/model-cp-epoch_0298.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 4.0217e-05\n",
      "Epoch 299/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0178e-05\n",
      "Epoch 00299: saving model to ./data/checkpoints/model-cp-epoch_0299.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0178e-05\n",
      "Epoch 300/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 4.0147e-05\n",
      "Epoch 00300: saving model to ./data/checkpoints/model-cp-epoch_0300.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 4.0150e-05\n",
      "Epoch 301/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9834e-05\n",
      "Epoch 00301: saving model to ./data/checkpoints/model-cp-epoch_0301.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 3.9835e-05\n",
      "Epoch 302/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9907e-05\n",
      "Epoch 00302: saving model to ./data/checkpoints/model-cp-epoch_0302.h5\n",
      "51403/51403 [==============================] - 455s 9ms/sample - loss: 3.9910e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9861e-05\n",
      "Epoch 00303: saving model to ./data/checkpoints/model-cp-epoch_0303.h5\n",
      "51403/51403 [==============================] - 453s 9ms/sample - loss: 3.9862e-05\n",
      "Epoch 304/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9790e-05\n",
      "Epoch 00304: saving model to ./data/checkpoints/model-cp-epoch_0304.h5\n",
      "51403/51403 [==============================] - 454s 9ms/sample - loss: 3.9794e-05\n",
      "Epoch 305/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9849e-05\n",
      "Epoch 00305: saving model to ./data/checkpoints/model-cp-epoch_0305.h5\n",
      "51403/51403 [==============================] - 454s 9ms/sample - loss: 3.9856e-05\n",
      "Epoch 306/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9708e-05\n",
      "Epoch 00306: saving model to ./data/checkpoints/model-cp-epoch_0306.h5\n",
      "51403/51403 [==============================] - 455s 9ms/sample - loss: 3.9709e-05\n",
      "Epoch 307/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9567e-05\n",
      "Epoch 00307: saving model to ./data/checkpoints/model-cp-epoch_0307.h5\n",
      "51403/51403 [==============================] - 455s 9ms/sample - loss: 3.9566e-05\n",
      "Epoch 308/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9631e-05\n",
      "Epoch 00308: saving model to ./data/checkpoints/model-cp-epoch_0308.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.9631e-05\n",
      "Epoch 309/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9772e-05\n",
      "Epoch 00309: saving model to ./data/checkpoints/model-cp-epoch_0309.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9772e-05\n",
      "Epoch 310/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9345e-05\n",
      "Epoch 00310: saving model to ./data/checkpoints/model-cp-epoch_0310.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9346e-05\n",
      "Epoch 311/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9365e-05\n",
      "Epoch 00311: saving model to ./data/checkpoints/model-cp-epoch_0311.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9364e-05\n",
      "Epoch 312/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9272e-05\n",
      "Epoch 00312: saving model to ./data/checkpoints/model-cp-epoch_0312.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9268e-05\n",
      "Epoch 313/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9235e-05\n",
      "Epoch 00313: saving model to ./data/checkpoints/model-cp-epoch_0313.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9236e-05\n",
      "Epoch 314/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9118e-05\n",
      "Epoch 00314: saving model to ./data/checkpoints/model-cp-epoch_0314.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9117e-05\n",
      "Epoch 315/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9044e-05\n",
      "Epoch 00315: saving model to ./data/checkpoints/model-cp-epoch_0315.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9043e-05\n",
      "Epoch 316/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9188e-05\n",
      "Epoch 00316: saving model to ./data/checkpoints/model-cp-epoch_0316.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.9186e-05\n",
      "Epoch 317/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.9000e-05\n",
      "Epoch 00317: saving model to ./data/checkpoints/model-cp-epoch_0317.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.8998e-05\n",
      "Epoch 318/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8906e-05\n",
      "Epoch 00318: saving model to ./data/checkpoints/model-cp-epoch_0318.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.8906e-05\n",
      "Epoch 319/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8723e-05\n",
      "Epoch 00319: saving model to ./data/checkpoints/model-cp-epoch_0319.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.8719e-05\n",
      "Epoch 320/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8773e-05\n",
      "Epoch 00320: saving model to ./data/checkpoints/model-cp-epoch_0320.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 3.8773e-05\n",
      "Epoch 321/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8923e-05\n",
      "Epoch 00321: saving model to ./data/checkpoints/model-cp-epoch_0321.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 3.8924e-05\n",
      "Epoch 322/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8684e-05\n",
      "Epoch 00322: saving model to ./data/checkpoints/model-cp-epoch_0322.h5\n",
      "51403/51403 [==============================] - 451s 9ms/sample - loss: 3.8688e-05\n",
      "Epoch 323/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8756e-05\n",
      "Epoch 00323: saving model to ./data/checkpoints/model-cp-epoch_0323.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 3.8758e-05\n",
      "Epoch 324/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8730e-05\n",
      "Epoch 00324: saving model to ./data/checkpoints/model-cp-epoch_0324.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 3.8731e-05\n",
      "Epoch 325/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8509e-05\n",
      "Epoch 00325: saving model to ./data/checkpoints/model-cp-epoch_0325.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 3.8513e-05\n",
      "Epoch 326/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8556e-05\n",
      "Epoch 00326: saving model to ./data/checkpoints/model-cp-epoch_0326.h5\n",
      "51403/51403 [==============================] - 450s 9ms/sample - loss: 3.8554e-05\n",
      "Epoch 327/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8386e-05\n",
      "Epoch 00327: saving model to ./data/checkpoints/model-cp-epoch_0327.h5\n",
      "51403/51403 [==============================] - 453s 9ms/sample - loss: 3.8388e-05\n",
      "Epoch 328/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8442e-05\n",
      "Epoch 00328: saving model to ./data/checkpoints/model-cp-epoch_0328.h5\n",
      "51403/51403 [==============================] - 453s 9ms/sample - loss: 3.8440e-05\n",
      "Epoch 329/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8239e-05\n",
      "Epoch 00329: saving model to ./data/checkpoints/model-cp-epoch_0329.h5\n",
      "51403/51403 [==============================] - 453s 9ms/sample - loss: 3.8240e-05\n",
      "Epoch 330/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8364e-05\n",
      "Epoch 00330: saving model to ./data/checkpoints/model-cp-epoch_0330.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.8365e-05\n",
      "Epoch 331/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8387e-05\n",
      "Epoch 00331: saving model to ./data/checkpoints/model-cp-epoch_0331.h5\n",
      "51403/51403 [==============================] - 458s 9ms/sample - loss: 3.8383e-05\n",
      "Epoch 332/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8265e-05\n",
      "Epoch 00332: saving model to ./data/checkpoints/model-cp-epoch_0332.h5\n",
      "51403/51403 [==============================] - 461s 9ms/sample - loss: 3.8262e-05\n",
      "Epoch 333/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7923e-05\n",
      "Epoch 00333: saving model to ./data/checkpoints/model-cp-epoch_0333.h5\n",
      "51403/51403 [==============================] - 457s 9ms/sample - loss: 3.7928e-05\n",
      "Epoch 334/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8317e-05\n",
      "Epoch 00334: saving model to ./data/checkpoints/model-cp-epoch_0334.h5\n",
      "51403/51403 [==============================] - 455s 9ms/sample - loss: 3.8318e-05\n",
      "Epoch 335/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7990e-05\n",
      "Epoch 00335: saving model to ./data/checkpoints/model-cp-epoch_0335.h5\n",
      "51403/51403 [==============================] - 453s 9ms/sample - loss: 3.7990e-05\n",
      "Epoch 336/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8048e-05\n",
      "Epoch 00336: saving model to ./data/checkpoints/model-cp-epoch_0336.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.8048e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8245e-05\n",
      "Epoch 00337: saving model to ./data/checkpoints/model-cp-epoch_0337.h5\n",
      "51403/51403 [==============================] - 455s 9ms/sample - loss: 3.8243e-05\n",
      "Epoch 338/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7774e-05\n",
      "Epoch 00338: saving model to ./data/checkpoints/model-cp-epoch_0338.h5\n",
      "51403/51403 [==============================] - 454s 9ms/sample - loss: 3.7776e-05\n",
      "Epoch 339/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7892e-05\n",
      "Epoch 00339: saving model to ./data/checkpoints/model-cp-epoch_0339.h5\n",
      "51403/51403 [==============================] - 453s 9ms/sample - loss: 3.7890e-05\n",
      "Epoch 340/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7816e-05\n",
      "Epoch 00340: saving model to ./data/checkpoints/model-cp-epoch_0340.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7819e-05\n",
      "Epoch 341/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.8059e-05\n",
      "Epoch 00341: saving model to ./data/checkpoints/model-cp-epoch_0341.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.8060e-05\n",
      "Epoch 342/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7595e-05\n",
      "Epoch 00342: saving model to ./data/checkpoints/model-cp-epoch_0342.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7596e-05\n",
      "Epoch 343/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7771e-05\n",
      "Epoch 00343: saving model to ./data/checkpoints/model-cp-epoch_0343.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7771e-05\n",
      "Epoch 344/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7469e-05\n",
      "Epoch 00344: saving model to ./data/checkpoints/model-cp-epoch_0344.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7471e-05\n",
      "Epoch 345/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7502e-05\n",
      "Epoch 00345: saving model to ./data/checkpoints/model-cp-epoch_0345.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7503e-05\n",
      "Epoch 346/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7518e-05\n",
      "Epoch 00346: saving model to ./data/checkpoints/model-cp-epoch_0346.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7516e-05\n",
      "Epoch 347/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7417e-05\n",
      "Epoch 00347: saving model to ./data/checkpoints/model-cp-epoch_0347.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7416e-05\n",
      "Epoch 348/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7334e-05\n",
      "Epoch 00348: saving model to ./data/checkpoints/model-cp-epoch_0348.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7335e-05\n",
      "Epoch 349/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7311e-05\n",
      "Epoch 00349: saving model to ./data/checkpoints/model-cp-epoch_0349.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7311e-05\n",
      "Epoch 350/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7295e-05\n",
      "Epoch 00350: saving model to ./data/checkpoints/model-cp-epoch_0350.h5\n",
      "51403/51403 [==============================] - 454s 9ms/sample - loss: 3.7296e-05\n",
      "Epoch 351/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7371e-05\n",
      "Epoch 00351: saving model to ./data/checkpoints/model-cp-epoch_0351.h5\n",
      "51403/51403 [==============================] - 461s 9ms/sample - loss: 3.7370e-05\n",
      "Epoch 352/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7420e-05\n",
      "Epoch 00352: saving model to ./data/checkpoints/model-cp-epoch_0352.h5\n",
      "51403/51403 [==============================] - 453s 9ms/sample - loss: 3.7421e-05\n",
      "Epoch 353/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7014e-05\n",
      "Epoch 00353: saving model to ./data/checkpoints/model-cp-epoch_0353.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7017e-05\n",
      "Epoch 354/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7444e-05\n",
      "Epoch 00354: saving model to ./data/checkpoints/model-cp-epoch_0354.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7442e-05\n",
      "Epoch 355/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7039e-05\n",
      "Epoch 00355: saving model to ./data/checkpoints/model-cp-epoch_0355.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7039e-05\n",
      "Epoch 356/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7050e-05\n",
      "Epoch 00356: saving model to ./data/checkpoints/model-cp-epoch_0356.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7053e-05\n",
      "Epoch 357/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.6925e-05\n",
      "Epoch 00357: saving model to ./data/checkpoints/model-cp-epoch_0357.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.6928e-05\n",
      "Epoch 358/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7135e-05\n",
      "Epoch 00358: saving model to ./data/checkpoints/model-cp-epoch_0358.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.7136e-05\n",
      "Epoch 359/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.6797e-05\n",
      "Epoch 00359: saving model to ./data/checkpoints/model-cp-epoch_0359.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.6797e-05\n",
      "Epoch 360/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.7150e-05\n",
      "Epoch 00360: saving model to ./data/checkpoints/model-cp-epoch_0360.h5\n",
      "51403/51403 [==============================] - 459s 9ms/sample - loss: 3.7150e-05\n",
      "Epoch 361/1000\n",
      "51392/51403 [============================>.] - ETA: 0s - loss: 3.6761e-05\n",
      "Epoch 00361: saving model to ./data/checkpoints/model-cp-epoch_0361.h5\n",
      "51403/51403 [==============================] - 452s 9ms/sample - loss: 3.6760e-05\n",
      "Epoch 362/1000\n",
      "51008/51403 [============================>.] - ETA: 3s - loss: 3.6969e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ab516664abc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINITIAL_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Speech-enhancement/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Documents/Speech-enhancement/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Documents/Speech-enhancement/env/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Speech-enhancement/env/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/Documents/Speech-enhancement/env/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = []\n",
    "\n",
    "name = \"model-cp-epoch_{epoch:04d}.h5\"\n",
    "path = os.path.join(checkpoints_path, name)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(path, verbose=1, monitor='val_loss', save_best_only=False, mode='auto', period=1)\n",
    "callbacks.append(checkpoint)\n",
    "\n",
    "log_dir = logs_dir + \"/model-{0}\".format(int(time.time()))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='epoch', write_graph=True, profile_batch=0)\n",
    "callbacks.append(tensorboard_callback)\n",
    "        \n",
    "model.fit(x=X, y=Y, batch_size=16, epochs=1000, verbose=1, callbacks=callbacks, shuffle=True, initial_epoch=INITIAL_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "thesis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
